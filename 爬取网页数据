from bs4 import BeautifulSoup
import requests
import re
import urllib
import time
import  xlwt

def saveData(datalist,savepath):
    workbook = xlwt.Workbook(encoding="utf-8",style_compression=0)#创建workbook对象
    worksheet = workbook.add_sheet('房价信息',cell_overwrite_ok=True)#cell:True:允许单元内容覆盖
    title=("房间名称","房间大小","房间位置","房间价格")
    for i in range(0,4):
        worksheet.write(0,i,title[i])
    for i in range(0,1050):
        data = datalist[i]
        for j in range(0,4):
            worksheet.write(i+1,j,data[j])
    #worksheet.write(0,0,'hello')#0行0列，写入数据
    workbook.save('房屋信息.xls')

def askURL(url):
    head = {
        "User-Agent": "Mozilla / 5.0(Windows NT 10.0; Win64; x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 80.0.3987.122  Safari / 537.36"
    }
#伪装为浏览器
    request = urllib.request.Request(url, headers=head)
    html = ""
    try:
        response = urllib.request.urlopen(request)
        html = response.read().decode("utf-8")#deconde处理格式问题，反正我是不想看乱码了
    except urllib.error.URLError as e:
        if hasattr(e, "code"):
            print(e.code)
        if hasattr(e, "reason"):
            print(e.reason)
    return html

findname=re.compile(r'<h3 class="property-content-title-name" title="(.*?)">')
findscale=re.compile(r'<p class="property-content-info-text">(.*?)</p>')
findaddress1=re.compile(r'<p class="property-content-info-comm-name">(.*?)</p>')
findaddress2=re.compile(r'<p class="property-content-info-comm-address">(.*?)</p>')
findPricing1=re.compile(r'<span class="property-price-total-num">(.*?)</span>')
findPricing2=re.compile(r'<span class="property-price-total-text">(.*?)</span>')

datalist=[]*4
for i in range(0,35):#简单循环实现翻页
    html = askURL("http://www.tiboo.cn/chushou/{}".format(i))  # 保存获取到的网页源码
        # 2.逐一解析数据
    soup = BeautifulSoup(html, "html.parser")

    soup=BeautifulSoup(html,'html.parser')
    time.sleep(1)
    print("//////////////////////////////////////第{}页爬取完成/////////////////////////////////////////////////////////".format(i+1))
    for i in  range(0,30):#同一个页面循环实现爬取多个‘property’的信息
        getitem=soup.find_all('div',class_='property')[i]

        getitem=str(getitem)
        #print(getitem[0])
        name = re.findall(findname,getitem) #标题名称
        #print(name)
        #datalist.append(name)

        scale = re.findall(findscale,getitem) #房间规模大小
        #print(scale)
        #datalist.append(scale)

        address1 = re.findall(findaddress1,getitem)
        address2 = re.findall(findaddress2,getitem)#房间地址
        address=address1+address2
        #print(address)
        #datalist.append(address)

        Pricing1= re.findall(findPricing1,getitem)
        Pricing2= re.findall(findPricing2,getitem)#定价
        Pricing=Pricing1+Pricing2
        #print(Pricing)
        #datalist.append(Pricing)
        datalist.append([name,scale,address,Pricing])
#print(datalist)
savepath = "房屋信息.xls"
saveData(datalist, savepath)  # 保存数据
print("爬取结束")
